---
title: è¯­éŸ³è¯†åˆ«
date: 2020-04-01 09:53:42
tags: é¡¹ç›®
categories: é¡¹ç›®
---

æ•´ä¸ªè¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸»è¦æ„æˆæœ‰å››ä¸ªéƒ¨åˆ†ï¼šè¯­éŸ³ä¿¡å·å¤„ç†å’Œç‰¹å¾çš„æå–æ¨¡å—ï¼›è¯­éŸ³è¯†åˆ«çš„å£°å­¦æ¨¡å‹ï¼›è¯­éŸ³è¯†åˆ«çš„è¯­è¨€æ¨¡å‹ï¼›è¯­éŸ³è¯†åˆ«çš„è§£ç å’Œæœç´¢éƒ¨åˆ†ã€‚ç³»ç»Ÿæµç¨‹å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](1.png)

<!--more-->
æ•´ä¸ªè¯­éŸ³è¯†åˆ«çš„è¿‡ç¨‹å¯ä»¥ç”¨è´å¶æ–¯ç†è®ºæ¥æè¿°ï¼Œå‡è®¾è¾“å…¥çš„éŸ³é¢‘åºåˆ—ä¸º$O={o_{1}, o_{2}, \cdots, o_{n}}$ï¼Œè¾“å‡ºä¸ºæ–‡æœ¬åºåˆ—ä¸º$W={w_{1}, w_{2}, \cdots, w_{n}}$ã€‚ç›®çš„æ˜¯æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å¾—$\prod_{n} P\left(W_{n} | O_{n}\right)$æœ€å¤§ï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒé›†ä¸­nä¸ªæ ·æœ¬çš„åéªŒæ¦‚ç‡æœ€å¤§ã€‚å•ç‹¬æ‹¿å‡ºä¸€ä¸ªæ ·æœ¬çš„åéªŒæ¦‚ç‡ä½¿ç”¨è´å¶æ–¯å…¬å¼å¯ä»¥å¾—åˆ°ï¼š

$$P(W | O)=\frac{P(O | W) * P(W)}{P(O)}$$

è¿›ä¸€æ­¥å¯å¾—:

$$W^{*}=\operatorname{argmax}_{W} P(O | W) * P(W)$$

è¿™é‡Œ$P(W)$æ˜¯è¾“å‡ºè¯åºåˆ—çš„æ¦‚ç‡ï¼Œç”¨è¯­è¨€æ¨¡å‹æ¥åˆ»ç”»ï¼Œ$P(O | W)$ä¸ºä¼¼ç„¶æ¦‚ç‡ï¼Œä½¿ç”¨å£°å­¦æ¨¡å‹æ¥è¡¨è¾¾ã€‚

### æ•°æ®é›†æ„å»º
è¯­éŸ³æ•°æ®æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤ç§ï¼Œä¸€æ˜¯å…¬å¼€æ•°æ®é›†â€”>æ—¥å¸¸å¯¹è¯ï¼ŒäºŒæ˜¯ç‰¹å®šæ•°æ®é›†â€”>æ§åˆ¶å‘½ä»¤ã€‚

æ‰€æœé›†çš„å…¬å¼€è¯­éŸ³æ•°æ®é›†æ˜¯[THCHS30](http://www.openslr.org/18/)ã€[ST-CMDS](http://cn-mirror.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz)ã€[AIShell](http://cn-mirror.openslr.org/resources/33/data_aishell.tgz)ç­‰æ•°æ®é›†ï¼Œè¿™å‡ ä¸ªæ•°æ®é›†æ˜¯ä¸­æ–‡è¯­éŸ³æ•°æ®é›†ï¼ŒåŸºæœ¬æ•°æ®æ ¼å¼å¦‚ä¸‹å›¾æ‰€ç¤º:

![](2.png)

åŸºæœ¬ä¿¡æ¯å¦‚ä¸‹ï¼š

![](3.png)

é’ˆå¯¹å®é™…çš„å·¥ä¸šæ§åˆ¶å‘½ä»¤ï¼Œæˆ‘ä»¬è¿›è¡Œä¸“é—¨çš„è¯­éŸ³é‡‡é›†å’Œå½•åˆ¶ã€‚ç¯å¢ƒæ˜¯å’Œå®é™…çš„æ³µç«™æ§åˆ¶å®¤ä¸€è‡´ï¼Œå®‰é™ã€è¿‘åœºã€‚è¿™é‡Œä½¿ç”¨å•ç²’éº¦å…‹é£è¿›è¡Œå½•åˆ¶ï¼Œå½•åˆ¶çš„æ ¼å¼ä¸ä¸Šè¾¹æ ¼å¼ä¸€è‡´ã€‚

![](4.png)

### è¯­éŸ³ä¿¡å·é¢„å¤„ç†

è¯­éŸ³ä¿¡å·é¢„å¤„ç†æ˜¯ä¸ºäº†å°†åŸå§‹è¯­éŸ³ä¿¡å·è½¬æ¢æˆCNNç½‘ç»œè¾“å…¥çš„è¯­è°±å›¾ã€‚CNNçš„è¾“å…¥å±‚æ˜¯200ç»´çš„ç‰¹å¾å€¼åºåˆ—ã€‚è¾“å‡ºæ‹¼éŸ³çš„è¡¨ç¤ºå¤§å°æ˜¯1422ï¼Œå³1421ä¸ªæ‹¼éŸ³+1ä¸ªç©ºç™½å—ã€‚é¢„æµ‹ç»“æœæ˜¯è¿”å›è¯­éŸ³è¯†åˆ«åçš„æ‹¼éŸ³ç¬¦å·åˆ—è¡¨ã€‚

è¯­éŸ³ä¿¡å·çš„é¢„å¤„ç†è¿‡ç¨‹ç”¨åˆ°çš„æŠ€æœ¯æ˜¯ï¼šé¢„åŠ é‡ï¼ˆpre-emphasisï¼‰ã€
åˆ†å¸§ï¼ˆenframingï¼‰ã€åŠ çª—ï¼ˆwindowing)

#### é¢„åŠ é‡

é¢„åŠ é‡å¯ä»¥ä½¿å¾—è¯­éŸ³ä¿¡å·çš„é¢‘è°±ä¿¡å·åˆ†å¸ƒæ›´åŠ å‡è¡¡ã€‚é¢„åŠ é‡ä¸€èˆ¬æ˜¯ç”¨
60db/octaveæ•°å­—æ»¤æ³¢å™¨é¢‘ç‡ç‰¹æ€§å®Œæˆçš„ã€‚è¿‡æ»¤å™¨å…¬å¼ç»™å‡ºï¼š

$$H(z)=1-u z^{-1}$$

å…¶ä¸­$u$æ˜¯é¢„åŠ é‡ç³»æ•°ã€‚


#### åˆ†å¸§

ä½¿ç”¨é¢„å…ˆè®¾è®¡å¥½çš„çª—å‡½æ•°å¤„ç†è¢«åˆ†å¸§åˆ†æˆçš„å¤šä¸ªå°ç‰‡æ®µã€‚é€šè¿‡è¿™ç§æ–¹æ³•åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³è¯­éŸ³ä¿¡å·å¸§ä¿¡å·ä¹‹é—´çš„ä¸è¿ç»­æ€§é—®é¢˜ã€‚æ¯ä¸€ä¸ªå°æ®µä½œä¸ºè¯­éŸ³ä¿¡çš„å•ä½ï¼Œç§°ä¸ºå¸§ï¼ˆframeï¼‰ã€‚ä»¥å¸§ä¸ºå•ä½å¯ä»¥åˆ†æè¯­éŸ³ä¿¡å·çš„çŸ­æ—¶ç‰¹æ€§ï¼Œä»è€Œè¿›ä¸€æ­¥è¿›è¡Œé¢‘è°±ç­‰å¤„ç†ã€‚åˆ†å¸§ä¸€èˆ¬ç›¸é‚»å¸§ä¹‹é—´æœ‰äº¤å‰é‡å ï¼Œäº¤å‰éƒ¨åˆ†å«åšå¸§ç§»ï¼Œå¸§ç§»ä¸€èˆ¬å®šä¸ºå¸§é•¿åº¦çš„ä¸€åŠã€‚

![](5.png)


#### åŠ çª—

å¯¹è¯­éŸ³ä¿¡å·å¢åŠ ä¸€ä¸ªçª—å‡½æ•°ï¼ˆWindow Functionï¼‰ï¼Œåœ¨è¯­éŸ³ä¿¡å·çš„ç»™å®šåŒºé—´å†…é™å®šä¸ºä¸€ä¸ªå®æ•°ï¼Œå…¶å®ƒéƒ¨åˆ†ä¸º0ã€‚ä½¿ç”¨çš„æ˜¯æ±‰æ˜çª—ï¼ˆHamming Windowï¼‰æ±‰æ˜çª—çš„æ—¶é—´åŸŸè¡¨è¾¾ç»™å‡º:

$$w(n)=0.54-0.46 \cos \left(\frac{2 \pi n}{N-1}\right)$$

å…¶ä¸­$ğ‘›$æ˜¯æ—¶é—´ï¼Œ$w(n)$æ˜¯çª—å‡½æ•°ã€‚

#### è¯­è°±å›¾ç‰¹å¾æå–

è¯­è°±å›¾ï¼ˆSpectrogramï¼‰æ˜¯è¯­éŸ³å¤„ç†çš„ä¸€ä¸ªé‡è¦ç‰¹å¾ï¼Œæ˜¯ä¸€ç§æè¿°è¯­éŸ³ä¿¡å·çš„å„ä¸ªé¢‘ç‡æˆåˆ†éšç€æ—¶é—´å˜æ¢çš„çƒ­åŠ›å›¾ã€‚

è¯­è°±å›¾åŒ…å«ç€è¯­éŸ³ä¿¡å·çš„é‡è¦ä¿¡æ¯ï¼Œç”¨ä¸€ä¸ªäºŒç»´çŸ©é˜µè¡¨ç¤ºã€‚å…¶ä¸­ä¸€ä¸ªè½´è¡¨ç¤ºæ—¶é—´ï¼Œå¦å¤–ä¸€ä¸ªè½´è¡¨ç¤ºé¢‘ç‡ã€‚ä¸åŒçš„ç‚¹æˆ–è€…é¢œè‰²ä»£è¡¨å¯¹åº”è¯­éŸ³ä¿¡å·èƒ½é‡çš„å¤§å°ã€‚

è¯­è°±å›¾çš„æå–è¿‡ç¨‹ä¸€èˆ¬åŒ…æ‹¬ï¼šé‡‡æ ·ã€å‚…ç«‹å¶å˜æ¢ã€è¿ç»­æ‹¼æ¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](6.jpg)


åŸå§‹è¯­éŸ³ä¿¡å·æ˜¯ç¬¦åˆæ•°æ®æ ¼å¼çš„åç¼€æ˜¯.wav çš„åŸå§‹è¯­éŸ³æ•°æ®ã€‚è¿™é‡Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯»å–è¯¥åŸå§‹è¯­éŸ³æ•°æ®ï¼Œè¿”å›å£°éŸ³ä¿¡å·çš„æ—¶åŸŸè°±çŸ©é˜µå’Œå¸§é€Ÿç‡ã€‚å…·ä½“å‡½æ•°å¦‚ä¸‹ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
ä¸€äº›é€šç”¨å‡½æ•°ï¼Œå¦‚wavæ–‡ä»¶è¯»å–ã€ä¿¡å·å‡ºæ¥å’Œæµ‹è¯•ä»£ç 
'''
import os
import wave
import numpy as np
import matplotlib.pyplot as plt  
import math
import time

from python_speech_features import mfcc
from python_speech_features import delta
from python_speech_features import logfbank

from scipy.fftpack import fft

def read_wav_data(filename):
	'''
	è¯»å–ä¸€ä¸ªwavæ–‡ä»¶ï¼Œè¿”å›å£°éŸ³ä¿¡å·çš„æ—¶åŸŸè°±çŸ©é˜µå’Œæ’­æ”¾æ—¶é—´
	'''
	wav = wave.open(filename,"rb") # æ‰“å¼€ä¸€ä¸ªwavæ ¼å¼çš„å£°éŸ³æ–‡ä»¶æµ
	num_frame = wav.getnframes() # è·å–å¸§æ•°
	num_channel=wav.getnchannels() # è·å–å£°é“æ•°
	framerate=wav.getframerate() # è·å–å¸§é€Ÿç‡
	num_sample_width=wav.getsampwidth() # è·å–å®ä¾‹çš„æ¯”ç‰¹å®½åº¦ï¼Œå³æ¯ä¸€å¸§çš„å­—èŠ‚æ•°
	str_data = wav.readframes(num_frame) # è¯»å–å…¨éƒ¨çš„å¸§
	wav.close() # å…³é—­æµ
	wave_data = np.fromstring(str_data, dtype = np.short) # å°†å£°éŸ³æ–‡ä»¶æ•°æ®è½¬æ¢ä¸ºæ•°ç»„çŸ©é˜µå½¢å¼
	wave_data.shape = -1, num_channel # æŒ‰ç…§å£°é“æ•°å°†æ•°ç»„æ•´å½¢ï¼Œå•å£°é“æ—¶å€™æ˜¯ä¸€åˆ—æ•°ç»„ï¼ŒåŒå£°é“æ—¶å€™æ˜¯ä¸¤åˆ—çš„çŸ©é˜µ
	wave_data = wave_data.T # å°†çŸ©é˜µè½¬ç½®
	return wave_data, framerate  

def GetMfccFeature(wavsignal, fs):
	# è·å–è¾“å…¥ç‰¹å¾
	feat_mfcc=mfcc(wavsignal[0],fs)
	feat_mfcc_d=delta(feat_mfcc,2)
	feat_mfcc_dd=delta(feat_mfcc_d,2)
	# è¿”å›å€¼åˆ†åˆ«æ˜¯mfccç‰¹å¾å‘é‡çš„çŸ©é˜µåŠå…¶ä¸€é˜¶å·®åˆ†å’ŒäºŒé˜¶å·®åˆ†çŸ©é˜µ
	wav_feature = np.column_stack((feat_mfcc, feat_mfcc_d, feat_mfcc_dd))
	return wav_feature

x=np.linspace(0, 400 - 1, 400, dtype = np.int64)
w = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (400 - 1) ) # æ±‰æ˜çª—

def GetFrequencyFeature3(wavsignal, fs):
	# wavæ³¢å½¢ åŠ æ—¶é—´çª—ä»¥åŠæ—¶ç§»10ms
	time_window = 25 # å•ä½ms
	window_length = fs / 1000 * time_window # è®¡ç®—çª—é•¿åº¦çš„å…¬å¼ï¼Œç›®å‰å…¨éƒ¨ä¸º400å›ºå®šå€¼
	
	wav_arr = np.array(wavsignal)
	#wav_length = len(wavsignal[0])
	wav_length = wav_arr.shape[1]
	
	range0_end = int(len(wavsignal[0])/fs*1000 - time_window) // 10 # è®¡ç®—å¾ªç¯ç»ˆæ­¢çš„ä½ç½®ï¼Œä¹Ÿå°±æ˜¯æœ€ç»ˆç”Ÿæˆçš„çª—æ•°
	data_input = np.zeros((range0_end, 200), dtype = np.float) # ç”¨äºå­˜æ”¾æœ€ç»ˆçš„é¢‘ç‡ç‰¹å¾æ•°æ®
	data_line = np.zeros((1, 400), dtype = np.float)
	
	for i in range(0, range0_end):
		p_start = i * 160
		p_end = p_start + 400
		
		data_line = wav_arr[0, p_start:p_end]
		
		data_line = data_line * w # åŠ çª—
		
		data_line = np.abs(fft(data_line)) / wav_length
		
		
		data_input[i]=data_line[0:200] # è®¾ç½®ä¸º400é™¤ä»¥2çš„å€¼ï¼ˆå³200ï¼‰æ˜¯å–ä¸€åŠæ•°æ®ï¼Œå› ä¸ºæ˜¯å¯¹ç§°çš„
		
	#print(data_input.shape)
	data_input = np.log(data_input + 1)
	return data_input

def wav_scale(energy):
	'''
	è¯­éŸ³ä¿¡å·èƒ½é‡å½’ä¸€åŒ–
	'''
	means = energy.mean() # å‡å€¼
	var=energy.var() # æ–¹å·®
	e=(energy-means)/math.sqrt(var) # å½’ä¸€åŒ–èƒ½é‡
	return e

	
def wav_show(wave_data, fs): # æ˜¾ç¤ºå‡ºæ¥å£°éŸ³æ³¢å½¢
	time = np.arange(0, len(wave_data)) * (1.0/fs)  # è®¡ç®—å£°éŸ³çš„æ’­æ”¾æ—¶é—´ï¼Œå•ä½ä¸ºç§’
	# ç”»å£°éŸ³æ³¢å½¢
	#plt.subplot(211)  
	plt.plot(time, wave_data)  
	#plt.subplot(212)  
	#plt.plot(time, wave_data[1], c = "g")  
	plt.show()  

	
def get_wav_list(filename):
	'''
	è¯»å–ä¸€ä¸ªwavæ–‡ä»¶åˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªå­˜å‚¨è¯¥åˆ—è¡¨çš„å­—å…¸ç±»å‹å€¼
	'''
	txt_obj=open(filename,'r') # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å…¥
	txt_text=txt_obj.read()
	txt_lines=txt_text.split('\n') # æ–‡æœ¬åˆ†å‰²
	dic_filelist={} # åˆå§‹åŒ–å­—å…¸
	list_wavmark=[] # åˆå§‹åŒ–wavåˆ—è¡¨
	for i in txt_lines:
		if(i!=''):
			txt_l=i.split(' ')
			dic_filelist[txt_l[0]] = txt_l[1]
			list_wavmark.append(txt_l[0])
	txt_obj.close()
	return dic_filelist,list_wavmark
	
def get_wav_symbol(filename):
	'''
	è¯»å–æŒ‡å®šæ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰wavæ–‡ä»¶å¯¹åº”çš„è¯­éŸ³ç¬¦å·
	è¿”å›ä¸€ä¸ªå­˜å‚¨ç¬¦å·é›†çš„å­—å…¸ç±»å‹å€¼
	'''
	txt_obj=open(filename,'r') # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å…¥
	txt_text=txt_obj.read()
	txt_lines=txt_text.split('\n') # æ–‡æœ¬åˆ†å‰²
	dic_symbol_list={} # åˆå§‹åŒ–å­—å…¸
	list_symbolmark=[] # åˆå§‹åŒ–symbolåˆ—è¡¨
	for i in txt_lines:
		if(i!=''):
			txt_l=i.split(' ')
			dic_symbol_list[txt_l[0]]=txt_l[1:]
			list_symbolmark.append(txt_l[0])
	txt_obj.close()
	return dic_symbol_list,list_symbolmark
	
if(__name__=='__main__'):
	
	wave_data, fs = read_wav_data("A2_0.wav")  
	
	wav_show(wave_data[0],fs)
	t0=time.time()
	freimg = GetFrequencyFeature(wave_data,fs)
	t1=time.time()
	print('time cost:',t1-t0)
	
	freimg = freimg.T
	plt.subplot(111)
	
	plt.imshow(freimg)
	plt.colorbar(cax=None,ax=None,shrink=0.5)  
	 
	plt.show() 
```

åŸå§‹è¯­éŸ³ä¿¡å·ç»è¿‡å¤„ç†åå¾—åˆ°å¸§æ•°ã€å£°é“æ•°ã€å¸§é€Ÿç‡ã€‚ç„¶åè¯»å–å…¨éƒ¨å¸§æ•°ï¼Œå†ç»è¿‡å˜æ¢åè½¬æ¢æˆæ•°ç»„çŸ©é˜µæ ¼å¼ï¼Œè¯¥æ•°ç»„æŒ‰ç…§å£°é“æ•°è¿›è¡Œæ•°ç»„æ•´å½¢ï¼Œæ•´å½¢åæ•°ç»„è½¬ç½®å¾—åˆ°æ—¶åŸŸè°±çŸ©é˜µï¼Œå¾—åˆ°å¸§é€Ÿç‡å’Œæ—¶åŸŸè°±çŸ©é˜µã€‚åœ¨å¾—åˆ°å¸§é€Ÿç‡å’Œæ—¶åŸŸè°±çŸ©é˜µåï¼Œç»è¿‡åŠ çª—ã€é‡‡æ ·åã€å‚…é‡Œå¶å˜æ¢ï¼Œä»¥åŠæœ€åçš„è¿ç»­æ‹¼æ¥ã€‚æµç¨‹å›¾å¦‚ä¸‹ï¼š

![](7.png)


ä¸‹å›¾æ˜¯é‡‡ç”¨ä¸Šè¿°ä»£ç å¯¹ä¸­æ–‡è¯­éŸ³æ•°æ®æå–çš„æ³¢å½¢å›¾å’Œè¯­è°±å›¾çš„ç»“æœã€‚æ³¢å½¢å›¾çš„æ¨ªè½´æ˜¯æ—¶é—´ï¼Œçºµè½´å¯ä»¥ç†è§£ä¸ºä½ç§»æˆ–è€…å‹å¼ºã€‚è¯­è°±å›¾çš„æ¨ªè½´æ˜¯æ‹¼æ¥çš„æ—¶é—´ï¼Œçºµè½´æ˜¯é¢‘ç‡ã€‚

![](8.png)


### å£°å­¦æ¨¡å‹è®¾è®¡è®­ç»ƒ

å£°å­¦æ¨¡å‹è¾“â¼Šå…¥æ˜¯è¯­è°±å›¾ï¼Œè¾“å‡ºæ˜¯æ‹¼éŸ³ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](9.png)

å£°å­¦æ¨¡å‹è®¾è®¡ä¸»è¦åŸºäºDFCNNç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä¸»è¦è®¾è®¡çš„ç»“æ„å’Œç›¸åº”çš„å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š

![](10.png)

å‚æ•°è®¾ç½®ï¼š

Adam çš„å‚æ•°:
(lr = 0.01, beta_1 = 0.9,beta_2 = 0.999, decay = 0.0, epsilon = 10e-8)

![](11.png)


CTCæ˜¯ä¸€ç§é’ˆå¯¹åºåˆ—ï¦œæ·±åº¦æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚ç›¸â½æ¯”ä¼ ç»Ÿäº¤å‰ç†µæŸå¤±å‡½æ•°èƒ½â¾ƒè‡ªåŠ¨å¯¹â»¬é½åºåˆ—ï¦œæ ‡ç­¾ï¼Œç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹ã€‚å…·ä½“å‚è€ƒ[è¿™é‡Œ](https://blog.ailemon.me/2019/07/18/sequence-modeling-with-ctc/)

ä½¿ç”¨Kerasæ­å»ºåºè´¯(Sequential)æ¨¡å‹ï¼Œéƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š
```python
#!/usr/local/bin/python
# -*- coding: utf-8 -*-
"""
å£°å­¦æ¨¡å‹
"""
import platform as plat
import os
import time

from general_function.file_wav import *
from general_function.file_dict import *
from general_function.gen_func import *
from general_function.muti_gpu import *
from keras.utils import multi_gpu_model,plot_model

import tensorflow as tf
import keras as kr
import numpy as np
import random

from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Input, Reshape, BatchNormalization # , Flatten
from keras.layers import Lambda, TimeDistributed, Activation,Conv2D, MaxPooling2D #, Merge
from keras import backend as K
from keras.optimizers import SGD, Adadelta, Adam, RMSprop

from ReadData import DataSpeech

abspath = ''
ModelName='_dfcnn'
#NUM_GPU = 2
base_count=0

class ModelSpeech(): # è¯­éŸ³æ¨¡å‹ç±»
	def __init__(self, datapath):
		'''
		åˆå§‹åŒ–
		é»˜è®¤è¾“å‡ºçš„æ‹¼éŸ³çš„è¡¨ç¤ºå¤§å°æ˜¯1434ï¼Œå³1433ä¸ªæ‹¼éŸ³+1ä¸ªç©ºç™½å—
		'''
		MS_OUTPUT_SIZE = 1434
		self.MS_OUTPUT_SIZE = MS_OUTPUT_SIZE # ç¥ç»ç½‘ç»œæœ€ç»ˆè¾“å‡ºçš„æ¯ä¸€ä¸ªå­—ç¬¦å‘é‡ç»´åº¦çš„å¤§å°
		#self.BATCH_SIZE = BATCH_SIZE # ä¸€æ¬¡è®­ç»ƒçš„batch
		self.label_max_string_length = 64
		self.AUDIO_LENGTH = 1600  ## 16s
		self.AUDIO_FEATURE_LENGTH = 200
		self._model, self.base_model = self.CreateModel() 
		
		self.datapath = datapath
		self.slash = ''
		system_type = plat.system() # ç”±äºä¸åŒçš„ç³»ç»Ÿçš„æ–‡ä»¶è·¯å¾„è¡¨ç¤ºä¸ä¸€æ ·ï¼Œéœ€è¦è¿›è¡Œåˆ¤æ–­
		if(system_type == 'Windows'):
			self.slash='\\' # åæ–œæ 
		elif(system_type == 'Linux'):
			self.slash='/' # æ­£æ–œæ 
		else:
			print('*[Message] Unknown System\n')
			self.slash='/' # æ­£æ–œæ 
		if(self.slash != self.datapath[-1]): # åœ¨ç›®å½•è·¯å¾„æœ«å°¾å¢åŠ æ–œæ 
			self.datapath = self.datapath + self.slash
	
		
	def CreateModel(self):
		'''
		å®šä¹‰DFCNNæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼æ¨¡å‹  
		è¾“å…¥å±‚ï¼š200ç»´çš„ç‰¹å¾å€¼åºåˆ—ï¼Œä¸€æ¡è¯­éŸ³æ•°æ®çš„æœ€å¤§é•¿åº¦è®¾ä¸º1600ï¼ˆå¤§çº¦16sï¼‰
		éšè—å±‚ï¼šå·ç§¯æ± åŒ–å±‚ï¼Œå·ç§¯æ ¸å¤§å°ä¸º3x3ï¼Œæ± åŒ–çª—å£å¤§å°ä¸º2
		éšè—å±‚ï¼šå…¨è¿æ¥å±‚
		è¾“å‡ºå±‚ï¼šå…¨è¿æ¥å±‚ï¼Œç¥ç»å…ƒæ•°é‡ä¸ºself.MS_OUTPUT_SIZEï¼Œä½¿ç”¨softmaxä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œ
		CTCå±‚ï¼šä½¿ç”¨CTCçš„lossä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå®ç°è¿æ¥æ€§æ—¶åºå¤šè¾“å‡º
		
		'''
		
		input_data = Input(name='the_input', shape=(self.AUDIO_LENGTH, self.AUDIO_FEATURE_LENGTH, 1))
		
		layer_h1 = Conv2D(32, (3,3), use_bias=False, activation='relu', padding='same', kernel_initializer='he_normal')(input_data) # å·ç§¯å±‚
		layer_h1 = BatchNormalization(mode=0,axis=-1)(layer_h1)
		layer_h2 = Conv2D(32, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h1) # å·ç§¯å±‚
		layer_h2 = BatchNormalization(axis=-1)(layer_h2)
		layer_h3 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h2) # æ± åŒ–å±‚ 800*100
		layer_h4 = Conv2D(64, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h3) # å·ç§¯å±‚
		layer_h4 = BatchNormalization(axis=-1)(layer_h4)
		layer_h5 = Conv2D(64, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h4) # å·ç§¯å±‚
		layer_h5 = BatchNormalization(axis=-1)(layer_h5)
		layer_h6 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h5) # æ± åŒ–å±‚ 400*50
		layer_h7 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h6) # å·ç§¯å±‚
		layer_h7 = BatchNormalization(axis=-1)(layer_h7)
		layer_h8 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h7) # å·ç§¯å±‚
		layer_h8 = BatchNormalization(axis=-1)(layer_h8)
		layer_h9 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h8) # æ± åŒ–å±‚ 200*25
		layer_h10 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h9) # å·ç§¯å±‚
		layer_h10 = BatchNormalization(axis=-1)(layer_h10)
		layer_h11 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h10) # å·ç§¯å±‚
		layer_h11 = BatchNormalization(axis=-1)(layer_h11)
		layer_h12 = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h11) # æ± åŒ–å±‚ 200*25
		
		layer_h13 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h12) # å·ç§¯å±‚
		layer_h13 = BatchNormalization(axis=-1)(layer_h13)
		layer_h14 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h13) # å·ç§¯å±‚
		layer_h14 = BatchNormalization(axis=-1)(layer_h14)
		layer_h15 = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h14) # æ± åŒ–å±‚ None*200*25*128
		
		layer_h16 = Reshape((200, 3200))(layer_h15)
		layer_h17 = Dense(128, activation="relu", use_bias=True, kernel_initializer='he_normal')(layer_h16) # å…¨è¿æ¥å±‚
		layer_h17 = BatchNormalization(axis=1)(layer_h17)
		layer_h18 = Dense(self.MS_OUTPUT_SIZE, use_bias=True, kernel_initializer='he_normal')(layer_h17) # å…¨è¿æ¥å±‚
		layer_h18 = BatchNormalization(axis=1)(layer_h18)
		y_pred = Activation('softmax', name='Activation0')(layer_h18)
		model_data = Model(inputs = input_data, outputs = y_pred)
		model_data.summary()
		
		labels = Input(name='the_labels', shape=[self.label_max_string_length], dtype='float32')
		input_length = Input(name='input_length', shape=[1], dtype='int64')
		label_length = Input(name='label_length', shape=[1], dtype='int64')
		# Keras doesn't currently support loss funcs with extra parameters
		# so CTC loss is implemented in a lambda layer
		
        # CTC
		loss_out = Lambda(self.ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])
		
		model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)
		
		opt = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, decay = 0.0, epsilon = 10e-8)
		#model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)
		model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = opt)
		
		
		# captures output of softmax so we can decode the output during visualization
		test_func = K.function([input_data], [y_pred])
		
		print('[*æç¤º] åˆ›å»ºæ¨¡å‹æˆåŠŸï¼Œæ¨¡å‹ç¼–è¯‘æˆåŠŸ')
		return model, model_data
		
	def ctc_lambda_func(self, args):
		y_pred, labels, input_length, label_length = args
		
		y_pred = y_pred[:, :, :]
		#y_pred = y_pred[:, 2:, :]
		return K.ctc_batch_cost(labels, y_pred, input_length, label_length)
	
	
	
	def TrainModel(self, datapath, epoch = 2, save_step = 1000, batch_size = 32, filename = abspath + 'model_speech/m' + ModelName + '/speech_model'+ModelName):
		'''
		è®­ç»ƒæ¨¡å‹
		å‚æ•°ï¼š
			datapath: æ•°æ®ä¿å­˜çš„è·¯å¾„
			epoch: è¿­ä»£è½®æ•°
			save_step: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
			filename: é»˜è®¤ä¿å­˜æ–‡ä»¶åï¼Œä¸å«æ–‡ä»¶åç¼€å
		'''
		data=DataSpeech(datapath, 'train')
		
		num_data = data.GetDataNum() # è·å–æ•°æ®çš„æ•°é‡
		
		yielddatas = data.data_genetator(batch_size, self.AUDIO_LENGTH)
		
		for epoch in range(epoch): # è¿­ä»£è½®æ•°
			print('[running] train epoch %d .' % epoch)
			n_step = 0 # è¿­ä»£æ•°æ®æ•°
			while True:
				try:
					print('[message] epoch %d . Have train datas %d+'%(epoch, n_step*save_step))
					# data_genetatoræ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°
					
					#self._model.fit_generator(yielddatas, save_step, nb_worker=2)
					self._model.fit_generator(yielddatas, save_step)
					n_step += 1
				except StopIteration:
					print('[error] generator error. please check data format.')
					break
				
				self.SaveModel(comment='_e_'+str(epoch)+'_step_'+str(n_step * save_step))
				self.TestModel(self.datapath, str_dataset='train', data_count = 4)
				self.TestModel(self.datapath, str_dataset='dev', data_count = 4)
				
	def LoadModel(self,filename = abspath + 'model_speech/m'+ModelName+'/speech_model'+ModelName+'.model'):
		'''
		åŠ è½½æ¨¡å‹å‚æ•°
		'''
		self._model.load_weights(filename)
		self.base_model.load_weights(filename + '.base')

	def SaveModel(self,filename = abspath + 'model_speech/m'+ModelName+'/speech_model'+ModelName,comment=''):
		'''
		ä¿å­˜æ¨¡å‹å‚æ•°
		'''
		self._model.save_weights(filename + comment + '.model')
		self.base_model.save_weights(filename + comment + '.model.base')

	def TestModel(self, datapath='', str_dataset='dev', data_count = 32,comment = '', out_report = False, show_ratio = True, io_step_print = 10, io_step_file = 10):
		'''
		æµ‹è¯•æ£€éªŒæ¨¡å‹æ•ˆæœ
		
		io_step_print
			ä¸ºäº†å‡å°‘æµ‹è¯•æ—¶æ ‡å‡†è¾“å‡ºçš„ioå¼€é”€ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´è¿™ä¸ªå‚æ•°æ¥å®ç°
		
		io_step_file
			ä¸ºäº†å‡å°‘æµ‹è¯•æ—¶æ–‡ä»¶è¯»å†™çš„ioå¼€é”€ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´è¿™ä¸ªå‚æ•°æ¥å®ç°
		
		'''
		data=DataSpeech(self.datapath, str_dataset)
		#data.LoadDataList(str_dataset) 
		num_data = data.GetDataNum() # è·å–æ•°æ®çš„æ•°é‡
		if(data_count <= 0 or data_count > num_data): # å½“data_countä¸ºå°äºç­‰äº0æˆ–è€…å¤§äºæµ‹è¯•æ•°æ®é‡çš„å€¼æ—¶ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨æ•°æ®æ¥æµ‹è¯•
			data_count = num_data
		
		try:
			ran_num = random.randint(0,num_data - 1) # è·å–ä¸€ä¸ªéšæœºæ•°
			
			words_num = 0
			word_error_num = 0
			
			nowtime = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))
			if(out_report == True):
				txt_obj = open(abspath+'Test_Report_' + str_dataset + '_' + nowtime + '.txt', 'w', encoding='UTF-8') # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å…¥
			
			txt = 'æµ‹è¯•æŠ¥å‘Š\næ¨¡å‹ç¼–å· ' + ModelName + '\n\n'
			for i in range(data_count):
				data_input, data_labels = data.GetData((ran_num + i) % num_data)  # ä»éšæœºæ•°å¼€å§‹è¿ç»­å‘åå–ä¸€å®šæ•°é‡æ•°æ®
				
				# æ•°æ®æ ¼å¼å‡ºé”™å¤„ç† å¼€å§‹
				# å½“è¾“å…¥çš„wavæ–‡ä»¶é•¿åº¦è¿‡é•¿æ—¶è‡ªåŠ¨è·³è¿‡è¯¥æ–‡ä»¶ï¼Œè½¬è€Œä½¿ç”¨ä¸‹ä¸€ä¸ªwavæ–‡ä»¶æ¥è¿è¡Œ
				num_bias = 0
				while(data_input.shape[0] > self.AUDIO_LENGTH):
					print('*[Error]','wave data lenghth of num',(ran_num + i) % num_data, 'is too long.','\n A Exception raise when test Speech Model.')
					num_bias += 1
					data_input, data_labels = data.GetData((ran_num + i + num_bias) % num_data)  # ä»éšæœºæ•°å¼€å§‹è¿ç»­å‘åå–ä¸€å®šæ•°é‡æ•°æ®
				# æ•°æ®æ ¼å¼å‡ºé”™å¤„ç† ç»“æŸ
				
				pre = self.Predict(data_input, data_input.shape[0] // 8)
				
				words_n = data_labels.shape[0] # è·å–æ¯ä¸ªå¥å­çš„å­—æ•°
				words_num += words_n # æŠŠå¥å­çš„æ€»å­—æ•°åŠ ä¸Š
				edit_distance = GetEditDistance(data_labels, pre) # è·å–ç¼–è¾‘è·ç¦»
				if(edit_distance <= words_n): # å½“ç¼–è¾‘è·ç¦»å°äºç­‰äºå¥å­å­—æ•°æ—¶
					word_error_num += edit_distance # ä½¿ç”¨ç¼–è¾‘è·ç¦»ä½œä¸ºé”™è¯¯å­—æ•°
				else: # å¦åˆ™è‚¯å®šæ˜¯å¢åŠ äº†ä¸€å †ä¹±ä¸ƒå…«ç³Ÿçš„å¥‡å¥‡æ€ªæ€ªçš„å­—
					word_error_num += words_n # å°±ç›´æ¥åŠ å¥å­æœ¬æ¥çš„æ€»å­—æ•°å°±å¥½äº†
				
				if((i % io_step_print == 0 or i == data_count - 1) and show_ratio == True):
					#print('æµ‹è¯•è¿›åº¦ï¼š',i,'/',data_count)
					print('Test Count: ',i,'/',data_count)
				
				txt = ''
				if(out_report == True):
					# if(i % io_step_file == 0 or i == data_count - 1):
					# 	txt_obj.write(txt)
					# 	txt = ''
					txt += str(i) + '\n'
					txt += 'True:\t' + str(data_labels) + '\n'
					txt += 'Pred:\t' + str(pre) + '\n'
					txt += '\n'
					txt_obj.write(txt)
				
			print('*[Test Result] Speech Recognition ' + str_dataset + ' set word error ratio: ', word_error_num / words_num * 100, '%')
			if(out_report == True):
				txt += '*[æµ‹è¯•ç»“æœ] è¯­éŸ³è¯†åˆ« ' + str_dataset + ' é›†è¯­éŸ³å•å­—é”™è¯¯ç‡ï¼š ' + str(word_error_num / words_num * 100) + ' %'
				txt_obj.write(txt)
				txt = ''
				txt_obj.close()
			
		except StopIteration:
			print('[Error] Model Test Error. please check data format.')
	
	def Predict(self, data_input, input_len):
		'''
		é¢„æµ‹ç»“æœ
		è¿”å›è¯­éŸ³è¯†åˆ«åçš„æ‹¼éŸ³ç¬¦å·åˆ—è¡¨
		'''
		
		batch_size = 1 
		in_len = np.zeros((batch_size),dtype = np.int32)
		
		in_len[0] = input_len
		
		x_in = np.zeros((batch_size, 1600, self.AUDIO_FEATURE_LENGTH, 1), dtype=np.float)
		
		for i in range(batch_size):
			x_in[i,0:len(data_input)] = data_input
		
		
		base_pred = self.base_model.predict(x = x_in)
		
		base_pred =base_pred[:, :, :]
		
		r = K.ctc_decode(base_pred, in_len, greedy = True, beam_width=100, top_paths=1)
		
		#print('r', r)
		
		r1 = K.get_value(r[0][0])
		#print('r1', r1)
		
		#r2 = K.get_value(r[1])
		#print(r2)
		
		r1=r1[0]
		
		return r1
		pass
	
	def RecognizeSpeech(self, wavsignal, fs):
		'''
		æœ€ç»ˆåšè¯­éŸ³è¯†åˆ«ç”¨çš„å‡½æ•°ï¼Œè¯†åˆ«ä¸€ä¸ªwavåºåˆ—çš„è¯­éŸ³
		'''
		
		#data = self.data
		#data = DataSpeech('E:\\è¯­éŸ³æ•°æ®é›†')
		#data.LoadDataList('dev')
		# è·å–è¾“å…¥ç‰¹å¾
		#data_input = GetMfccFeature(wavsignal, fs)
		#t0=time.time()
		data_input = GetFrequencyFeature(wavsignal, fs)
		#t1=time.time()
		#print('time cost:',t1-t0)
		
		input_length = len(data_input)
		input_length = input_length // 8
		
		data_input = np.array(data_input, dtype = np.float)
		#print(data_input,data_input.shape)
		data_input = data_input.reshape(data_input.shape[0],data_input.shape[1],1)
		#t2=time.time()
		r1 = self.Predict(data_input, input_length)
		#t3=time.time()
		#print('time cost:',t3-t2)
		list_symbol_dic = GetSymbolList(self.datapath) # è·å–æ‹¼éŸ³åˆ—è¡¨
		
		r_str=[]
		for i in r1:
			r_str.append(list_symbol_dic[i])
		
		return r_str
		pass
		
	def RecognizeSpeech_FromFile(self, filename):
		'''
		æœ€ç»ˆåšè¯­éŸ³è¯†åˆ«ç”¨çš„å‡½æ•°ï¼Œè¯†åˆ«æŒ‡å®šæ–‡ä»¶åçš„è¯­éŸ³
		'''
		
		wavsignal,fs = read_wav_data(filename)
		
		r = self.RecognizeSpeech(wavsignal, fs)
		
		return r
		
		pass
		
	
	@property
	def model(self):
		'''
		è¿”å›keras model
		'''
		return self._model
```

### è¯­è¨€æ¨¡å‹è®¾è®¡è®­ç»ƒ
### å®éªŒåˆ†æå’Œè½¯ä»¶è®¾è®¡